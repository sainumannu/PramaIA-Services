# TEST SUITE GUIDE

Guida rapida alla test suite di PramaIA - Batteria di test per validazione e debug.

## ğŸ“ Ubicazione

```
tests/
â”œâ”€â”€ __init__.py                    # Package marker
â”œâ”€â”€ conftest.py                    # pytest configuration e fixtures
â”œâ”€â”€ pytest.ini                     # pytest settings
â”œâ”€â”€ requirements.txt               # dipendenze test
â”œâ”€â”€ test_utils.py                  # Utility condivise
â”œâ”€â”€ test_inventory.py              # Inventario sistema
â”œâ”€â”€ test_crud_operations.py        # Operazioni CRUD
â”œâ”€â”€ test_e2e_pipeline.py          # Test end-to-end
â”œâ”€â”€ run_tests.py                   # Test runner (Python)
â”œâ”€â”€ run_tests.ps1                  # Test runner (PowerShell)
â””â”€â”€ TEST_SUITE_README.md           # Documentazione completa
```

## ğŸš€ Quick Start

### 1. Installare Dipendenze
```bash
cd tests
pip install -r requirements.txt
```

### 2. Eseguire Test
```bash
# Tutti i test
pytest tests/ -v -s

# Inventory (cosa c'Ã¨ nel sistema)
pytest tests/test_inventory.py -v -s

# CRUD (creare/leggere/aggiornare/eliminare)
pytest tests/test_crud_operations.py -v -s

# End-to-End (flusso completo)
pytest tests/test_e2e_pipeline.py -v -s
```

### 3. Con Script Runner
```bash
# Python
python tests/run_tests.py inventory -v -s

# PowerShell
.\tests\run_tests.ps1 -Suite crud -Verbose -ShowOutput
```

## ğŸ“Š Cosa Testano

### test_inventory.py - âœ… Inventario Sistema
Verifica quanti e quali componenti sono disponibili:
- âœ… Workflow configurati
- âœ… Nodi PDK disponibili
- âœ… Event source registrati
- âœ… Trigger configurati
- âœ… Documenti nel sistema

**Output**: Tabelle con lista completa componenti

### test_crud_operations.py - ğŸ“ Operazioni CRUD
Testa operazioni di creazione, lettura, aggiornamento, eliminazione:
- âœ… Creare documento
- âœ… Leggere metadati
- âœ… Aggiornare metadata
- âœ… Eliminare documenti
- âœ… Operazioni VectorStore
- âœ… IntegritÃ  database

**Output**: Verifiche che CRUD funziona correttamente

### test_e2e_pipeline.py - ğŸ”„ End-to-End
Testa il flusso completo da file a search:
- âœ… Monitoraggio folder
- âœ… Processamento eventi
- âœ… Esecuzione trigger/workflow
- âœ… Embedding documento
- âœ… Ricerca semantica
- âœ… Sincronizzazione DB-VectorStore

**Output**: Verifica che il flusso completo funziona

## ğŸ”§ Configurazione

### Variabili d'Ambiente
```bash
# Backend
BACKEND_URL=http://127.0.0.1:8000
DATABASE_URL=sqlite:///./PramaIAServer/backend/data/database.db

# PDK Server
PDK_SERVER_BASE_URL=http://127.0.0.1:3001

# VectorStore
VECTORSTORE_SERVICE_URL=http://127.0.0.1:8090

# Monitor Agent
MONITOR_AGENT_URL=http://127.0.0.1:8001

# LogService
PRAMAIALOG_URL=http://127.0.0.1:8081
```

## ğŸ“ˆ Utility e Helper

### Nella suite
- `ServiceHealthCheck.check_all()` - Verifica tutti i servizi
- `APIClient.get/post/put/delete()` - HTTP request
- `DatabaseHelper.query/execute()` - Query database
- `TestDataGenerator.generate_*()` - Crea dati test
- `TestReporter.print_*()` - Formatta output

## ğŸ› Debugging

### Se fallisce un test
1. Verificare servizi: `pytest tests/test_inventory.py::TestWorkflowInventory::test_get_workflows_from_api -v`
2. Controllare logs: porta 8081
3. Verificare database: `sqlite3 database.db "SELECT COUNT(*) FROM workflows"`

### Comandi Utili
```bash
# Solo test falliti precedentemente
pytest tests/ --lf

# Stop al primo fallimento
pytest tests/ -x

# Verbose error
pytest tests/ --tb=long

# Specifico test
pytest tests/test_inventory.py::TestWorkflowInventory -v
```

## ğŸ“ Aggiungere Nuovi Test

### Template
```python
from test_utils import APIClient, DatabaseHelper, TestReporter

class TestNewFeature:
    def test_my_feature(self):
        TestReporter.print_header("MY FEATURE TEST")
        
        # Test code
        response = APIClient.get("http://127.0.0.1:8000/api/endpoint")
        assert response.status_code == 200
        
        TestReporter.print_result("Result", "âœ…")
```

## ğŸ“Š Output Formato

I test producono output formattato:
```
================================================================================
  TEST NAME
================================================================================

âœ… Result: value
ğŸ“‹ Data:
  - Item 1
  - Item 2

ğŸ“Š Statistics:
Workflow | Active | Created
---------|--------|--------
wf1      | True   | 2025-11-18
```

## ğŸ¯ Best Practices

1. **Eseguire dopo deployment**: `pytest tests/ -v`
2. **Eseguire prima di merge**: `pytest tests/ -x`
3. **Conservare report**: `pytest tests/ --html=report.html`
4. **Aggiornare quando cambiano API**: Mantenere test sincronizzati

## âš™ï¸ Configurazione Test

File: `pytest.ini`
```ini
[pytest]
addopts = -v -s
timeout = 300
markers = 
    inventory: Inventario test
    crud: CRUD test
    e2e: End-to-end test
```

## ğŸ”„ Continuous Integration

Test suite Ã¨ progettata per CI/CD:

```yaml
# GitHub Actions example
- name: Run Tests
  run: pytest tests/ -v --html=report.html

- name: Upload Report
  uses: actions/upload-artifact@v2
  with:
    name: test-report
    path: report.html
```

## ğŸ“š Fixture Disponibili

In conftest.py:
- `check_services`: Verifica servizi all'inizio
- `test_data`: Generator di dati test
- `test_session`: Tracking risultati

## ğŸš¨ Stato Atteso

| Scenario | Comportamento | Soluzione |
|----------|---------------|-----------|
| Servizi offline | Test skipped | Avviare servizi |
| Database vuoto | Test passa (dati generati) | Normale |
| VectorStore offline | Fallback a database | Avviare vectorstore |
| Trigger non eseguito | Test falisce | Verificare config trigger |

---

**Vedi**: `tests/TEST_SUITE_README.md` per documentazione completa  
**Aggiornato**: 18 Novembre 2025
